{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:11:18.416958Z",
     "start_time": "2025-04-07T13:11:17.848445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all functions from the required modules\n",
    "from cordo_sherpa_module import *\n",
    "from plot_module import *\n",
    "#from association_module import *\n",
    "print(\"Successfully loaded all modules\")"
   ],
   "id": "b3bd2b16-0f23-45e2-afdd-86ff1d0bf677",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded plotting command\n",
      "Successfully loaded all modules\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:11:19.217422Z",
     "start_time": "2025-04-07T13:11:18.921366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths to the files\n",
    "path_fichier_shp = \"data/2-output-data/donnees_shp\"\n",
    "title_shp = \"donnees_paris_donnees_insee_iris\"\n",
    "path_fichier_pourcents = \"data/2-output-data\"\n",
    "title_pourcents = \"pourcents\"\n",
    "\n",
    "# Load the concentration points\n",
    "conc_points = coordo_sherpa(sc=\"s1\", pol=\"ug_NO2\", year=2019)\n",
    "\n",
    "# Load the exported data\n",
    "donnees_exportees = gpd.read_file(os.path.join(path_fichier_shp, f\"{title_shp}.shp\"))\n",
    "\n",
    "# Transform the CRS of the exported data to match the concentration points\n",
    "donnees_exportees_transformed = donnees_exportees.to_crs(conc_points.crs)\n",
    "\n",
    "# Check if CRSs are the same\n",
    "if conc_points.crs == donnees_exportees_transformed.crs:\n",
    "    print(\"CRS for conc_points_transformed and donnees_exportees_transformed are the same.\")\n",
    "else:\n",
    "    print(\"CRS for conc_points_transformed and donnees_exportees_transformed are different.\")"
   ],
   "id": "f885f9bd-ecf7-4bf0-bbef-c932ff16819b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concentrations in 2019 and 2019 are calculated for the pollutant 'ug_no2' (s1).\n",
      "CRS for conc_points_transformed and donnees_exportees_transformed are the same.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:11:20.401183Z",
     "start_time": "2025-04-07T13:11:20.392867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def generate_points(polygone, conc_points):\n",
    "    \"\"\"\n",
    "    Generate grid points for a given polygon and associate them with concentration points.\n",
    "    \"\"\"\n",
    "    if polygone.geometry.is_empty or not polygone.geometry.is_valid:\n",
    "        return gpd.GeoDataFrame()  # Return empty GeoDataFrame if invalid\n",
    "\n",
    "    if conc_points is None or conc_points.empty:\n",
    "        return gpd.GeoDataFrame()  # Return empty GeoDataFrame if conc_points is empty\n",
    "\n",
    "    # Filter concentration points within the polygon\n",
    "    points_within = conc_points[conc_points.geometry.within(polygone.geometry)].copy()\n",
    "    points_within['iriscod'] = polygone['iriscod']\n",
    "\n",
    "    return points_within\n",
    "\n",
    "def calculate_intersection_percentages(grille_combinee, donnees_exportees_transformed):\n",
    "    \"\"\"\n",
    "    Iteratively calculate the intersection percentages for each grid point's rectangle and associated polygon.\n",
    "    Returns:\n",
    "        - GeoDataFrame: grille_combinee with calculated 'perc' values.\n",
    "    \"\"\"\n",
    "    # Initialize 'perc' column with zeros\n",
    "    grille_combinee['perc'] = 0\n",
    "\n",
    "    # Iterate through each row in grille_combinee\n",
    "    for idx, row in grille_combinee.iterrows():\n",
    "        # Extract coordinates of the point\n",
    "        x, y = row.geometry.x, row.geometry.y\n",
    "\n",
    "        # Create a rectangle (polygon) around the point\n",
    "        rectangle = Polygon([\n",
    "            (round(x - 0.05, 3), round(y - 0.025, 3)),\n",
    "            (round(x + 0.05, 3), round(y - 0.025, 3)),\n",
    "            (round(x + 0.05, 3), round(y + 0.025, 3)),\n",
    "            (round(x - 0.05, 3), round(y + 0.025, 3)),\n",
    "            (round(x - 0.05, 3), round(y - 0.025, 3))\n",
    "        ])\n",
    "\n",
    "        # Filter the polygon corresponding to the current grid point's 'iriscod'\n",
    "        iriscod = row['iriscod']\n",
    "        polygon_row = donnees_exportees_transformed[donnees_exportees_transformed['iriscod'] == iriscod]\n",
    "\n",
    "        if polygon_row.empty or polygon_row.geometry.is_empty.any():\n",
    "            continue  # Skip if no matching polygon or geometry is invalid\n",
    "\n",
    "        # Extract the polygon geometry\n",
    "        polygon_geom = polygon_row.geometry.iloc[0]\n",
    "\n",
    "        # Intersection between the rectangle and the polygon\n",
    "        intersection = rectangle.intersection(polygon_geom)\n",
    "\n",
    "        if not intersection.is_empty:\n",
    "            area_intersection = intersection.area\n",
    "            area_polygon = polygon_geom.area\n",
    "\n",
    "            # Calculate the percentage of intersection\n",
    "            perc = round(area_intersection / area_polygon, 3)\n",
    "            grille_combinee.at[idx, 'perc'] = perc\n",
    "\n",
    "    # Filter rows where 'perc' is greater than 0\n",
    "    grille_combinee = grille_combinee[grille_combinee['perc'] > 0]\n",
    "\n",
    "    return grille_combinee\n",
    "\n",
    "from geopandas import GeoDataFrame  # Ensure correct imports\n",
    "def process_subset(subset, conc_points):\n",
    "    # Make sure subset and conc_points are correct types\n",
    "    if not isinstance(subset, GeoDataFrame):\n",
    "        raise TypeError(f\"Expected 'subset' to be a GeoDataFrame, got {type(subset)}\")\n",
    "    if not isinstance(conc_points, GeoDataFrame):\n",
    "        raise TypeError(f\"Expected 'conc_points' to be a GeoDataFrame, got {type(conc_points)}\")\n",
    "\n",
    "    # Perform operations on the 'subset' that involve iterating over rows\n",
    "    # Here, I'm assuming 'conc_points' needs to be used within the process\n",
    "    result_rows = []\n",
    "    for idx, row in subset.iterrows():\n",
    "        # Perform some operation with `row` and `conc_points`\n",
    "        result_rows.append(row)  # This is just a placeholder for actual logic\n",
    "\n",
    "    # Convert the result back to a GeoDataFrame\n",
    "    return GeoDataFrame(result_rows)\n",
    "\n",
    "\n",
    "\n",
    "def worker_function(donnees_subset, conc_points):\n",
    "    \"\"\"\n",
    "    Wrapper to process subsets of polygons using multiprocessing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_subset(donnees_subset, conc_points)\n",
    "    except Exception as e:\n",
    "        print(f\"Worker function error: {e}\")\n",
    "        return gpd.GeoDataFrame()"
   ],
   "id": "e5baba1cf5cf0a9b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:13:24.595563Z",
     "start_time": "2025-04-07T13:13:24.590257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from geopandas import GeoDataFrame\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def split_into_subsets(data, num_splits=4):\n",
    "    \"\"\"\n",
    "    Splits a GeoDataFrame into subsets for multiprocessing.\n",
    "    Args:\n",
    "        data (GeoDataFrame): The GeoDataFrame to be split.\n",
    "        num_splits (int): The number of subsets to create.\n",
    "    Returns:\n",
    "        List[GeoDataFrame]: A list of GeoDataFrame subsets.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return []  # Return an empty list if the input data is empty\n",
    "\n",
    "    # Split the data into evenly sized chunks\n",
    "    chunk_size = max(1, len(data) // num_splits)\n",
    "    return [data.iloc[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "\n",
    "def worker_func(subset, conc_points):\n",
    "    \"\"\"\n",
    "    Worker function to process a subset of data.\n",
    "    \"\"\"\n",
    "    # Add error-handling or debug statements inside worker_func\n",
    "    if subset.empty:\n",
    "        return GeoDataFrame()  # Return an empty GeoDataFrame for empty input\n",
    "\n",
    "    # Some complex processing logic here which returns a GeoDataFrame\n",
    "    # Example: spatial join, buffering, intersection, etc.\n",
    "    try:\n",
    "        processed_result = some_spatial_operations(subset, conc_points)\n",
    "        return processed_result\n",
    "    except Exception as e:\n",
    "        # Log the error for debugging purposes\n",
    "        print(f\"Error processing subset: {e}\")\n",
    "        return GeoDataFrame()  # Ensure the function always returns a GeoDataFrame\n",
    "\n",
    "\n",
    "def association(donnees_exportees_transformed, conc_points):\n",
    "    \"\"\"\n",
    "    This function performs a spatial association between two geospatial datasets.\n",
    "    \"\"\"\n",
    "    # Split the data into subsets for multiprocessing\n",
    "    subsets = split_into_subsets(donnees_exportees_transformed)\n",
    "\n",
    "    if not subsets:  # Check if subsets is empty before proceeding\n",
    "        raise ValueError(\"The input data resulted in empty subsets. Please check the input.\")\n",
    "\n",
    "    # Use multiprocessing only if subsets are valid\n",
    "    with Pool() as pool:\n",
    "        # Pass conc_points as an additional argument to worker_func\n",
    "        grids = pool.starmap(worker_func, [(subset, conc_points) for subset in subsets])\n",
    "\n",
    "    # Remove empty results from grids before concatenating\n",
    "    non_empty_grids = [grid for grid in grids if not grid.empty]\n",
    "\n",
    "    if not non_empty_grids:\n",
    "        # Raise error if there's no valid data after processing\n",
    "        raise ValueError(\"All subsets returned empty results. Check worker_func and input data.\")\n",
    "\n",
    "    # Concatenate the results into a single GeoDataFrame\n",
    "    result = GeoDataFrame(pd.concat(non_empty_grids, ignore_index=True))\n",
    "\n",
    "    return result"
   ],
   "id": "30694963a0ee0c95",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "scrolled": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-07T13:13:28.493263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GLOBAL FUNCTION FOR MULTIPROCESSING\n",
    "def worker_function(subset, conc_points):\n",
    "    return process_subset(subset, conc_points)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Paths and parameters\n",
    "    path_fichier_shp = \"data/2-output-data/donnees_shp\"\n",
    "    title_shp = \"donnees_insee_iris\"\n",
    "    path_fichier_pourcents = \"data/2-output-data\"\n",
    "    title_pourcents = \"pourcents_shp\"\n",
    "\n",
    "    # Step 1: Load exported shapefile data\n",
    "    donnees_exportees_transformed = gpd.read_file(os.path.join(path_fichier_shp, f\"{title_shp}.shp\"))\n",
    "\n",
    "    # Step 2: Load concentration points\n",
    "    conc_points = coordo_sherpa(sc=\"s1\", pol=\"ug_NO2\", year=2019)\n",
    "\n",
    "    # Step 3: Ensure CRS compatibility\n",
    "    donnees_exportees_transformed = donnees_exportees_transformed.to_crs(conc_points.crs)\n",
    "\n",
    "    # Check CRS validity\n",
    "    if donnees_exportees_transformed.crs != conc_points.crs:\n",
    "        print(\"CRS mismatch detected; converting CRS...\")\n",
    "        conc_points = conc_points.to_crs(donnees_exportees_transformed.crs)\n",
    "    else:\n",
    "        print(\"CRS for conc_points and donnees_exportees_transformed are the same.\")\n",
    "\n",
    "    # Step 4: Perform association\n",
    "    donnees_pourcents = association(donnees_exportees_transformed, conc_points)\n",
    "\n",
    "    # Step 5: Export shapefile\n",
    "    path_result = os.path.join(path_fichier_pourcents, f\"{title_pourcents}.shp\")\n",
    "    donnees_pourcents.to_file(path_result, driver='ESRI Shapefile')\n",
    "\n",
    "    print(f\"Results saved to {path_result}\")\n",
    "\n",
    "    # Step 6 (Optional): Visualization\n",
    "    try:\n",
    "        donnees_pourcents.plot(column='perc', cmap='viridis', legend=True)\n",
    "        plt.title(\"Intersection Percentages\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during plotting: {e}\")\n"
   ],
   "id": "c3b62dfa-5dc5-4466-b9d8-e61052936b36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concentrations in 2019 and 2019 are calculated for the pollutant 'ug_no2' (s1).\n",
      "CRS for conc_points and donnees_exportees_transformed are the same.\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
